#Workflow for Tuning

##Step 1: load module & environment
```
$ module load anaconda3
$ source activate desi\_sbenzvi\_lab
```

##Step 2: submit preprocessing script
This will read in coadd and truth fits files, decimate and normalize
them, partition them into x\_train, y\_train and x\_test, y\_test.
x\_test and y\_test are 10% of the total data and used for validation.

Prior versions did not do this explicitly and used validation\_split
within model.fit.

Submit the job to slurm using:
```
$ python submit_preprocessing.py -x
```
This will run preprocess\_data.py.

The rest of the code depends on these files, so it must be done 
sequentially, although the next are done in parallel.
Saves the following within the directory:
* x\_train.fits
* y\_train.fits
* x\_test.fits
* y\_test.fits
* preprocessing.sh: batch file for preprocessing that is run
* preprocessing.log: log file for preprocessing
* cnn/: directory for output files generated
Once the fits files are created, we can start the tuning script.

##Step 3: submit tuning
Running submit\_tuning.py will do the random search on the hyperparameters
in parallel.
This will submit n jobs to generate n CNNs by running 3LabelCNNTuning.py
with n random parameters.

Few parameter options for submit\_tuning.py
* -x: if provided will actually run all of the sh files and create log files
* --noweights: if provided, won't output hdf5 files
* --noevents: if provided, won't output tensorboard files
* --num\_iters %d: provide an integer n for the number of iterations/CNNs to 
generate, default=100

```
$ python submit_tuning.py -x --noevents --num_iters 50
```

##Step 4: Filter models
Running filter\_models.py will serach through all of the generated CNNs 
(the hist.json files) within the provided 'batch\_time' name and create a
list of all potential trained CNNs to look through. It saves the output
file, filter\_out.txt, within the corresponding batch({time}) file   

Few parameter options for submit\_tuning.py
* -batch\_time %s: required, provide the batch you want to search through. 
(Most likely the batch last created)
* --min\_dropout %f: minimum dropout rate for filtering (default=0.4)
* --min\_acc %f: minimum accuracy rate for filtering (default=0.95)
* --output\_file %s: name of output file, default= "filter\_out.txt"
```
$ python filter_models.py --batch_time "05-30_12:14:43"
$ cat cnn/categorical/batch\(05-30_12\:14\:43\)/filter_out.txt 
```

#Structure of cnn/ directory
* cnn/categorical/: main directory
* cnn/categorical/batch({time}): holds batch of n CNNs generated at the time 
submit\_tuning was ran
* cnn/categorical/batch({time})/scripts: holds n batch files generated by 
submit\_tuning
* cnn/categorical/batch({time}/logs: holds n of the log files created while
running the batch files in /scripts
* cnn/categorical/batch({time})/iter({i})\_run({time\_w\_microseconds}): holds
 all of files about the information of the CNN that was trained.
	* weights/: holds hdf5 files (if --noweights not provided)
	* tensorboard/: holds event file (if --noevents not provided)
	* hist.json: holds json file with CNN's overview
* cnn/categorical/batch({time})/filter\_out.txt: default name for filtered iteration numbers after running filter\_models.py


