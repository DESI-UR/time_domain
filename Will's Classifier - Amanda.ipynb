{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/software/python3/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/software/python3/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/software/python3/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/software/python3/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/software/python3/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/software/python3/3.6.5/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import regularizers, callbacks\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import (Input, Dense, Activation, ZeroPadding1D, \n",
    "BatchNormalization, Flatten, Reshape, Conv1D, MaxPooling1D, Dropout,Add, LSTM,Embedding)\n",
    "from keras.initializers import glorot_normal, glorot_uniform\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, load_model\n",
    "# from desispec.interpolation import resample_flux\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=plt.cm.tab10.colors)\n",
    "# plt.rcParamsDefault['axes.prop_cycle']\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['mathtext.fontset'] = 'dejavuserif'\n",
    "plt.rc('grid',alpha=0.3,linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def network(input_shape, learning_rate=0.0005, reg=0.0032, dropout=0.7436, seed=None):\n",
    "    \"\"\" \n",
    "    Args:\n",
    "    input_shape -- shape of the input spectra\n",
    "    regularization_strength -- regularization factor\n",
    "    dropout -- dropout rate\n",
    "    seed -- seed of initializer\n",
    "    Returns:\n",
    "    model -- a Model() instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    X_input = Input(input_shape, name='Input_Spec')\n",
    "\n",
    "    with K.name_scope('Conv_1'):\n",
    "        X = Conv1D(filters=8, kernel_size=5, strides=1, padding='same',\n",
    "                   kernel_regularizer=regularizers.l2(reg),\n",
    "                   bias_initializer='zeros',\n",
    "                   kernel_initializer=glorot_normal(seed))(X_input)\n",
    "        X = BatchNormalization(axis=2)(X)\n",
    "        X = Activation('relu')(X)\n",
    "        X = MaxPooling1D(pool_size= 2)(X)\n",
    "\n",
    "    with K.name_scope('Conv_2'):\n",
    "        X = Conv1D(filters=16, kernel_size=5, strides=1, padding='same',\n",
    "                   kernel_regularizer=regularizers.l2(reg),\n",
    "                   bias_initializer='zeros',\n",
    "                   kernel_initializer=glorot_normal(seed))(X)\n",
    "        X = BatchNormalization(axis=2)(X)\n",
    "        X = Activation('relu')(X)\n",
    "        X = MaxPooling1D(2)(X)\n",
    "    with K.name_scope('Conv_3'):\n",
    "        X = Conv1D(filters=32, kernel_size=5, strides=1, padding='same',\n",
    "                   kernel_regularizer=regularizers.l2(reg),\n",
    "                   bias_initializer='zeros',\n",
    "                   kernel_initializer=glorot_normal(seed))(X)\n",
    "        X = BatchNormalization(axis=2)(X)\n",
    "        X = Activation('relu')(X)\n",
    "        X = MaxPooling1D(2)(X)\n",
    "        \n",
    "    with K.name_scope('Conv_4'):\n",
    "        X = Conv1D(filters=64, kernel_size=5, strides=1, padding='same',\n",
    "                   kernel_regularizer=regularizers.l2(reg),\n",
    "                   bias_initializer='zeros',\n",
    "                   kernel_initializer=glorot_normal(seed))(X)\n",
    "        X = BatchNormalization(axis=2)(X)\n",
    "        X = Activation('relu')(X)\n",
    "        X = MaxPooling1D(2)(X)\n",
    "\n",
    "        \n",
    "    # FLATTEN -> FULLYCONNECTED\n",
    "    with K.name_scope('Dense_Layer'):\n",
    "        X = Flatten()(X)\n",
    "        X = Dense(256, kernel_regularizer=regularizers.l2(reg),\n",
    "                  activation='relu')(X)\n",
    "        X = Dropout(rate=dropout, seed=seed)(X)\n",
    "    \n",
    "    with K.name_scope('Output_Layer'):\n",
    "        X = Dense(1, kernel_regularizer=regularizers.l2(reg),\n",
    "              activation='sigmoid',name='Output_Classes')(X)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs=X, name='SNnet')\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = network((400,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_host = np.sort(glob.glob('hosts/*.fits'))\n",
    "flux_host = []\n",
    "for f in files_host:\n",
    "    h = fits.open(f)\n",
    "    fl = h['BRZ_FLUX'].data\n",
    "    flux_host.append(fl)\n",
    "fluxes_hosts_all = np.concatenate(flux_host)\n",
    "fluxes_hosts=[]\n",
    "bad_host_counter=0\n",
    "for i in fluxes_hosts_all:\n",
    "    if np.sum(i)==0 or math.isnan(np.sum(i)):\n",
    "        bad_host_counter=bad_host_counter+1\n",
    "    else:\n",
    "        fluxes_hosts.append(i)\n",
    "fluxes_hosts=np.asarray(fluxes_hosts)\n",
    "\n",
    "subspec_hosts = np.mean(fluxes_hosts[:,:6000].reshape(-1,400,15),2)\n",
    "maxflux = fluxes_hosts.max(axis=-1).reshape(-1,1)\n",
    "minflux = fluxes_hosts.min(axis=-1).reshape(-1,1)\n",
    "standarized_hosts = (subspec_hosts - minflux)/(maxflux-minflux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = np.sort(glob.glob('sneia/*.fits'))\n",
    "flux = []\n",
    "for f in files:\n",
    "    h = fits.open(f)\n",
    "    f = h['BRZ_FLUX'].data\n",
    "    zeros = np.zeros(400)\n",
    "    flux.append(f)\n",
    "fluxes_all = np.concatenate(flux)\n",
    "fluxes=[]\n",
    "bad_SN_counter=0\n",
    "for i in fluxes_all:\n",
    "    if np.sum(i)==0 or math.isnan(np.sum(i)):\n",
    "        bad_SN_counter=bad_SN_counter+1\n",
    "    else:\n",
    "        fluxes.append(i)\n",
    "fluxes=np.asarray(fluxes)\n",
    "\n",
    "subspec = np.mean(fluxes[:,:6000].reshape(-1,400,15),2)\n",
    "maxflux = subspec.max(axis=-1).reshape(-1,1)\n",
    "minflux = subspec.min(axis=-1).reshape(-1,1)\n",
    "standarized = (subspec - minflux)/(maxflux-minflux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate([standarized,standarized_hosts]).reshape(-1,400,1)\n",
    "y_train = np.concatenate([np.zeros(standarized.shape[0]),np.ones(standarized_hosts.shape[0])])\n",
    "permute = np.random.permutation(y_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17939 samples, validate on 1994 samples\n",
      "Epoch 1/30\n",
      "17939/17939 [==============================] - 29s 2ms/step - loss: 0.9689 - acc: 0.9907 - val_loss: 0.4961 - val_acc: 1.0000\n",
      "Epoch 2/30\n",
      "17939/17939 [==============================] - 26s 1ms/step - loss: 0.3206 - acc: 0.9999 - val_loss: 0.2000 - val_acc: 1.0000\n",
      "Epoch 3/30\n",
      "17939/17939 [==============================] - 26s 1ms/step - loss: 0.1454 - acc: 0.9997 - val_loss: 0.1047 - val_acc: 1.0000\n",
      "Epoch 4/30\n",
      "17939/17939 [==============================] - 26s 1ms/step - loss: 0.0792 - acc: 0.9999 - val_loss: 0.0657 - val_acc: 0.9995\n",
      "Epoch 5/30\n",
      " 3200/17939 [====>.........................] - ETA: 20s - loss: 0.0621 - acc: 0.9997"
     ]
    }
   ],
   "source": [
    "hist = model2.fit(x_train[permute][:30000],y_train[permute][:30000],batch_size=64,epochs=30,\n",
    "                  validation_split=0.1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['val_acc'])\n",
    "plt.show\n",
    "plt.title(\"val_acc vs epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['acc'])\n",
    "plt.show\n",
    "plt.title(\"acc vs epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.6.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
